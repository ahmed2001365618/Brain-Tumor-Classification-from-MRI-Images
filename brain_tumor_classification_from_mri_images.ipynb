{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNnto8w4H6uMW91PtcwyLY/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mcgmed/Brain-Tumor-Classification-from-MRI-Images/blob/main/brain_tumor_classification_from_mri_images.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_x06TVuVB_br",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf726093-0b3c-41bc-a688-ef8efb6c05e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.0/321.0 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.25.2 which is incompatible.\n",
            "tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.25.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "! pip3 install --upgrade --quiet google-cloud-aiplatform \\\n",
        "                                 google-cloud-storage \\\n",
        "                                 pillow \\\n",
        "                                 numpy\n",
        "\n",
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_ID = \"project-id\"\n",
        "REGION = \"us-central1\"\n",
        "\n",
        "! gcloud config set project {PROJECT_ID}"
      ],
      "metadata": {
        "id": "mW99NRA8Ce0r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85679209-1d56-463e-a6d4-838bc956535a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "L8W664_0CvJx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BUCKET_URI = f\"gs://bucket-{PROJECT_ID}\"\n",
        "! gsutil mb -l $REGION -p $PROJECT_ID $BUCKET_URI"
      ],
      "metadata": {
        "id": "C1FqFu5gCzg7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
      ],
      "metadata": {
        "id": "ALVWQgpnDRBK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_VERSION = \"tf-cpu.2-9\"\n",
        "DEPLOY_VERSION = \"tf2-cpu.2-9\"\n",
        "\n",
        "TRAIN_IMAGE = \"us-docker.pkg.dev/vertex-ai/training/{}:latest\".format(TRAIN_VERSION)\n",
        "DEPLOY_IMAGE = \"us-docker.pkg.dev/vertex-ai/prediction/{}:latest\".format(DEPLOY_VERSION)"
      ],
      "metadata": {
        "id": "2dIrC1LADi7R"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "JOB_NAME = \"brain_tumor_classification_from_mri_images\"\n",
        "MODEL_DIR = \"{}/{}\".format(BUCKET_URI, JOB_NAME)\n",
        "\n",
        "TRAIN_STRATEGY = \"single\"\n",
        "EPOCHS = 200\n",
        "\n",
        "CMDARGS = [\n",
        "    \"--epochs=\" + str(EPOCHS),\n",
        "    \"--distribute=\" + TRAIN_STRATEGY,\n",
        "]"
      ],
      "metadata": {
        "id": "3Mx6S2nDGLyT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile task.py\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import pathlib\n",
        "import argparse\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--lr', dest='lr',\n",
        "                    default=0.001, type=float,\n",
        "                    help='Learning rate.')\n",
        "parser.add_argument('--epochs', dest='epochs',\n",
        "                    default=200, type=int,\n",
        "                    help='Number of epochs.')\n",
        "parser.add_argument('--steps', dest='steps',\n",
        "                    default=4570//200, type=int,\n",
        "                    help='Number of steps per epoch.')\n",
        "parser.add_argument('--distribute', dest='distribute', type=str, default='single',\n",
        "                    help='distributed training strategy')\n",
        "args = parser.parse_args()\n",
        "\n",
        "print('Python Version = {}'.format(sys.version))\n",
        "print('TensorFlow Version = {}'.format(tf.__version__))\n",
        "print('TF_CONFIG = {}'.format(os.environ.get('TF_CONFIG', 'Not found')))\n",
        "print('DEVICES', device_lib.list_local_devices())\n",
        "\n",
        "# Single Machine, single compute device\n",
        "if args.distribute == 'single':\n",
        "    if tf.test.is_gpu_available():\n",
        "        strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
        "    else:\n",
        "        strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n",
        "# Single Machine, multiple compute device\n",
        "elif args.distribute == 'mirror':\n",
        "    strategy = tf.distribute.MirroredStrategy()\n",
        "# Multiple Machine, multiple compute device\n",
        "elif args.distribute == 'multi':\n",
        "    strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n",
        "\n",
        "# Multi-worker configuration\n",
        "print('num_replicas_in_sync = {}'.format(strategy.num_replicas_in_sync))\n",
        "\n",
        "# Preparing dataset\n",
        "BATCH_SIZE = 32\n",
        "IMG_HEIGHT = 180\n",
        "IMG_WIDTH = 180\n",
        "\n",
        "TRAINING_DATA_DIR = 'gs://bucket-atomic-griffin-394806/brain-tumor-mri-dataset/Training/'\n",
        "\n",
        "NUM_WORKERS = strategy.num_replicas_in_sync\n",
        "# Here the batch size scales up by number of workers since\n",
        "# `tf.data.Dataset.batch` expects the global batch size.\n",
        "GLOBAL_BATCH_SIZE = BATCH_SIZE * NUM_WORKERS\n",
        "\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  TRAINING_DATA_DIR,\n",
        "  validation_split=0.2,\n",
        "  subset=\"training\",\n",
        "  seed=123,\n",
        "  image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "  batch_size=GLOBAL_BATCH_SIZE)\n",
        "\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  TRAINING_DATA_DIR,\n",
        "  validation_split=0.2,\n",
        "  subset=\"validation\",\n",
        "  seed=123,\n",
        "  image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "  batch_size=GLOBAL_BATCH_SIZE)\n",
        "\n",
        "# Class Weights Calculation\n",
        "\n",
        "class_names = train_ds.class_names\n",
        "class_name_to_label = {class_name: idx for idx, class_name in enumerate(class_names)}\n",
        "class_counts = {}\n",
        "\n",
        "for _, labels in train_ds:\n",
        "    for label in labels.numpy():\n",
        "        class_name = class_names[label]\n",
        "        class_counts[class_name] = class_counts.get(class_name, 0) + 1\n",
        "\n",
        "label_list = []\n",
        "for _, labels in train_ds:\n",
        "    label_list.extend(labels.numpy())\n",
        "\n",
        "total_samples = len(label_list)\n",
        "class_weights = {class_name_to_label[class_name]: total_samples / count for class_name, count in class_counts.items()}\n",
        "sum_class_weights = sum(class_weights.values())\n",
        "class_weights = {label: weight / sum_class_weights for label, weight in class_weights.items()}\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "# Build the Keras model\n",
        "def build_and_compile_cnn_model():\n",
        "  data_augmentation = tf.keras.Sequential([\n",
        "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "    tf.keras.layers.RandomRotation(0.1),\n",
        "    tf.keras.layers.RandomZoom(0.1)\n",
        "  ])\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Rescaling(1./255, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
        "    data_augmentation,\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dropout(0.25),\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dropout(0.25),\n",
        "    tf.keras.layers.Conv2D(128, (5,5), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dropout(0.25),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(256, activation='relu', kernel_regularizer='l2'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dropout(0.25),\n",
        "    tf.keras.layers.Dense(256, activation='relu', kernel_regularizer='l2'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dropout(0.25),\n",
        "    tf.keras.layers.Dense(256, activation='relu', kernel_regularizer='l2'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(4, activation='softmax')\n",
        "  ])\n",
        "\n",
        "  def precision(y_true, y_pred):\n",
        "    true_positives = tf.keras.backend.sum(tf.keras.backend.round(tf.keras.backend.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = tf.keras.backend.sum(tf.keras.backend.round(tf.keras.backend.clip(y_pred, 0, 1)))\n",
        "    return true_positives / (predicted_positives + tf.keras.backend.epsilon())\n",
        "\n",
        "  def recall(y_true, y_pred):\n",
        "    true_positives = tf.keras.backend.sum(tf.keras.backend.round(tf.keras.backend.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = tf.keras.backend.sum(tf.keras.backend.round(tf.keras.backend.clip(y_true, 0, 1)))\n",
        "    return true_positives / (possible_positives + tf.keras.backend.epsilon())\n",
        "\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy(), precision, recall])\n",
        "\n",
        "  return model\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.001, patience=10, restore_best_weights=True)\n",
        "lr_sch = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=5, factor=0.1, verbose=1, min_lr=5e-10)\n",
        "\n",
        "# Train the model\n",
        "MODEL_DIR = os.getenv(\"AIP_MODEL_DIR\")\n",
        "\n",
        "with strategy.scope():\n",
        "    model = build_and_compile_cnn_model()\n",
        "\n",
        "model.fit(train_ds, validation_data=val_ds,\n",
        "          epochs=args.epochs,\n",
        "          class_weight=class_weights,\n",
        "          callbacks=[early_stopping, lr_sch])\n",
        "\n",
        "model.save(MODEL_DIR)"
      ],
      "metadata": {
        "id": "HMC5y-uoHDL8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a8b5742-65cd-4cc9-fbf8-9120ab8c179c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing task.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "job = aiplatform.CustomTrainingJob(\n",
        "    display_name=JOB_NAME,\n",
        "    script_path=\"task.py\",\n",
        "    container_uri=TRAIN_IMAGE,\n",
        "    model_serving_container_image_uri=DEPLOY_IMAGE,\n",
        ")\n",
        "\n",
        "MODEL_DISPLAY_NAME = \"brain_mri_tumor_classification\"\n",
        "\n",
        "model = job.run(\n",
        "    model_display_name=MODEL_DISPLAY_NAME,\n",
        "    args=CMDARGS,\n",
        "    replica_count=1,\n",
        ")"
      ],
      "metadata": {
        "id": "vRLZKJ1DcECQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEPLOYED_NAME = \"brain_mri_classification_deployed\"\n",
        "TRAFFIC_SPLIT = {\"0\": 100}\n",
        "MIN_NODES = 1\n",
        "MAX_NODES = 1\n",
        "\n",
        "endpoint = model.deploy(\n",
        "    deployed_model_display_name=DEPLOYED_NAME,\n",
        "    traffic_split=TRAFFIC_SPLIT,\n",
        "    min_replica_count=MIN_NODES,\n",
        "    max_replica_count=MAX_NODES,\n",
        ")"
      ],
      "metadata": {
        "id": "SpDO0nsQcayh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! gsutil -m cp -r gs://bucket-project-id/brain-tumor-mri-dataset/Testing/glioma/Te-glTr_0000.jpg .\n",
        "! gsutil -m cp -r gs://bucket-project-id/brain-tumor-mri-dataset/Training/meningioma/Tr-meTr_0000.jpg .\n",
        "! gsutil -m cp -r gs://bucket-project-id/brain-tumor-mri-dataset/Testing/notumor/Te-noTr_0000.jpg .\n",
        "! gsutil -m cp -r gs://bucket-project-id/brain-tumor-mri-dataset/Testing/pituitary/Te-piTr_0000.jpg ."
      ],
      "metadata": {
        "id": "oq904h8sZPIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_HEIGHT = 180\n",
        "IMG_WIDTH = 180\n",
        "BATCH_SIZE=32\n",
        "example='Tr-meTr_0000.jpg'\n",
        "\n",
        "def predict_image(example):\n",
        "  img = tf.keras.utils.load_img(example, target_size=(IMG_HEIGHT, IMG_WIDTH))\n",
        "  img_array = tf.keras.utils.img_to_array(img)\n",
        "  img_array = tf.expand_dims(img_array, 0)\n",
        "  sample = img_array.numpy().tolist() # Vertex AI endpoints do not accept ndarray and array types as input. Rather, you need to convert it into a python list.\n",
        "\n",
        "  class_names = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
        "  prediction = endpoint.predict(sample)\n",
        "  score = tf.nn.softmax(prediction[0])\n",
        "  print('This image most likely belongs to \"{}\" with a {:.2f} percent confidence.'.format(class_names[np.argmax(score)], 100 * np.max(score)))"
      ],
      "metadata": {
        "id": "MQgOw52WiTeZ"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example = 'Te-glTr_0000.jpg'\n",
        "predict_image(example)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5Ge_PY6HnvM",
        "outputId": "3516c8c5-e4bc-43d6-abbd-bfb9e84bfc11"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This image most likely belongs to \"glioma\" with a 34.17 percent confidence.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example = 'Tr-meTr_0000.jpg'\n",
        "predict_image(example)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAV4SWq_Htiy",
        "outputId": "ce1eb9fc-ab12-4968-e24e-f3ce16e1cb49"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This image most likely belongs to \"meningioma\" with a 32.82 percent confidence.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example = 'Te-noTr_0000.jpg'\n",
        "predict_image(example)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAxnhHUEHvQD",
        "outputId": "08c2b705-ca98-41b0-e700-9634625f665d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This image most likely belongs to \"notumor\" with a 35.79 percent confidence.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example = 'Te-piTr_0000.jpg'\n",
        "predict_image(example)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvXaxy2rHv8q",
        "outputId": "79f6aba0-f9c2-46ae-9cec-65f680042582"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This image most likely belongs to \"pituitary\" with a 47.46 percent confidence.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "endpoint.undeploy_all()\n",
        "\n",
        "delete_bucket = False\n",
        "\n",
        "# Delete the training job\n",
        "job.delete()\n",
        "\n",
        "endpoint.delete()\n",
        "model.delete()\n",
        "\n",
        "#if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
        "#    ! gsutil rm -r $BUCKET_URI"
      ],
      "metadata": {
        "id": "asi8uvhdD3JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('continue')"
      ],
      "metadata": {
        "id": "lFUZFJZ7sxNS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b4fbc8e-01a5-4ba8-e7ff-3150461494f3"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "continue\n"
          ]
        }
      ]
    }
  ]
}